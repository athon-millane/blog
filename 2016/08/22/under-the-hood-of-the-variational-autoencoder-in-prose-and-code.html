<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />
    
<title>Under the Hood of the Variational Autoencoder (in Prose and Code)</title>
<meta property="og:title" content="Under the Hood of the Variational Autoencoder (in Prose and Code)">
<meta property="description" content="The Variational Autoencoder (VAE) neatly synthesizes unsupervised deep learning and variational Bayesian methods into one sleek package. In Part I of this series, we introduced the theory and intuition behind the VAE, an exciting development in machine learning for combined generative modeling and inference—“machines that imagine and reason.”
">
<meta property="og:description" content="The Variational Autoencoder (VAE) neatly synthesizes unsupervised deep learning and variational Bayesian methods into one sleek package. In Part I of this series, we introduced the theory and intuition behind the VAE, an exciting development in machine learning for combined generative modeling and inference—“machines that imagine and reason.”
">
<meta property="og:image" content="https://blog.fastforwardlabs.comhttp://fastforwardlabs.github.io/blog-images/miriam/imgs_code/160816_1754_reloaded_latent_784_500_500_50_round_65536_morph_4730816952.gif">
<meta property="og:url" content="https://blog.fastforwardlabs.com/2016/08/22/under-the-hood-of-the-variational-autoencoder-in-prose-and-code.html">
<meta property="twitter:card" content="summary_large_image">

    <link rel="stylesheet" type="text/css" href="/style.css" />
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-53030428-5', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

  </head>
  <body>
      <div class="container">
        <div class="spacer"></div>
        <div style="height: 1.5rem; padding-top: 0.35rem;">
          <a target="_blank" href="https://www.cloudera.com/products/fast-forward-labs-research.html">
            <img style="height: 0.8rem;" src="/images/cloudera-fast-forward-logo.png" />
          </a>
        </div>
        <div class="spacer"></div>
      </div>
      <main id="main">
        
<div class="container">
  <div>
    <h3 class="clear"><a href="/">Blog</a></h3>
  </div>
  <div class="spacer"></div>
  <div class="post">
    <h5 class="clear">
      <span>Aug 22, 2016</span> &middot;
      <span style="text-transform: capitalize;">
        whitepaper
      </span>
    </h5>
    <h1>Under the Hood of the Variational Autoencoder (in Prose and Code)</h1>
    <h5 id="the-a-hrefhttpsarxivorgabs13126114variationala-a-hrefhttpsarxivorgabs14014082autoencodera-vae-neatly-synthesizes-unsupervised-deep-learning-and-variational-bayesian-methods-into-one-sleek-package-in-a-hrefhttpblogfastforwardlabscom20160812introducing-variational-autoencoders-in-prose-andhtmlpart-ia-of-this-series-we-introduced-the-theory-and-intuition-behind-the-vae-an-exciting-development-in-machine-learning-for-combined-generative-modeling-and-inferencea-hrefhttpshakirmcomslidesdlsummerschool_aug2016_compresspdfmachines-that-imagine-and-reasona">The <!-- raw HTML omitted -->Variational<!-- raw HTML omitted --> <!-- raw HTML omitted -->Autoencoder<!-- raw HTML omitted --> (VAE) neatly synthesizes unsupervised deep learning and variational Bayesian methods into one sleek package. In <!-- raw HTML omitted -->Part I<!-- raw HTML omitted --> of this series, we introduced the theory and intuition behind the VAE, an exciting development in machine learning for combined generative modeling and inference—<!-- raw HTML omitted -->“machines that imagine and reason.”<!-- raw HTML omitted --></h5>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>from functional import compose, partial
import numpy as np
import tensorflow as tf</p>
<pre><code>
&lt;p&gt;One perk of these models is their modularity—VAEs are naturally amenable to swapping in whatever encoder/decoder architecture is most fitting for the task at hand: &lt;a href=&quot;https://arxiv.org/abs/1502.04623&quot;&gt;recurrent&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/1511.06349&quot;&gt;neural&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/1412.6581&quot;&gt;networks&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1411.5928&quot;&gt;convolutional&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1503.03167&quot;&gt;deconvolutional&lt;/a&gt; networks, etc.&lt;/p&gt;
&lt;p&gt;For our purposes, we will model the relatively simple &lt;a href=&quot;http://yann.lecun.com/exdb/mnist/&quot;&gt;MNIST&lt;/a&gt; dataset using densely-connected layers, wired symmetrically around the hidden code.&lt;/p&gt;

```python
class Dense():
    &quot;&quot;&quot;Fully-connected layer&quot;&quot;&quot;
    def __init__(self, scope=&quot;dense_layer&quot;, size=None, dropout=1.,
                 nonlinearity=tf.identity):
        # (str, int, (float | tf.Tensor), tf.op)
        assert size, &quot;Must specify layer size (num nodes)&quot;
        self.scope = scope
        self.size = size
        self.dropout = dropout # keep_prob
        self.nonlinearity = nonlinearity

    def __call__(self, x):
        &quot;&quot;&quot;Dense layer currying, to apply layer to any input tensor `x`&quot;&quot;&quot;
        # tf.Tensor -&amp;gt; tf.Tensor
        with tf.name_scope(self.scope):
            while True:
                try: # reuse weights if already initialized
                    return self.nonlinearity(tf.matmul(x, self.w) + self.b)
                except(AttributeError):
                    self.w, self.b = self.wbVars(x.get_shape()[1].value, self.size)
                    self.w = tf.nn.dropout(self.w, self.dropout)
    ...
</code></pre><!-- raw HTML omitted -->
<pre><code>i.e. composed = composeAll([f, g, h])
     composed(x) # == f(g(h(x)))
&quot;&quot;&quot;
# adapted from https://docs.python.org/3.1/howto/functional.html
return partial(functools.reduce, compose)(*args)
</code></pre>
<pre><code>
&lt;p&gt;Now that we’ve defined our model primitives, we can tackle the VAE itself.&lt;/p&gt;
&lt;p&gt;Keep in mind: the TensorFlow computational graph is cleanly divorced from the numerical computations themselves. In other words, a &lt;code&gt;tf.Graph&lt;/code&gt; wireframes the underlying skeleton of the model, upon which we may hang values only within the context of a &lt;code&gt;tf.Session&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Below, we initialize class &lt;code&gt;VAE&lt;/code&gt; and activate a session for future convenience (so we can initialize and evaluate tensors within a single session, e.g. to persist weights and biases across rounds of training).&lt;/p&gt;
&lt;p&gt;Here are some relevant snippets, cobbled together from the &lt;a href=&quot;https://github.com/fastforwardlabs/vae-tf/blob/master/vae.py&quot;&gt;full source code&lt;/a&gt;:&lt;/p&gt;

```python
class VAE():
    &quot;&quot;&quot;Variational Autoencoder

    see: Kingma &amp;amp; Welling - Auto-Encoding Variational Bayes
    (https://arxiv.org/abs/1312.6114)
    &quot;&quot;&quot;
    DEFAULTS = {
        &quot;batch_size&quot;: 128,
        &quot;learning_rate&quot;: 1E-3,
        &quot;dropout&quot;: 1., # keep_prob
        &quot;lambda_l2_reg&quot;: 0.,
        &quot;nonlinearity&quot;: tf.nn.elu,
        &quot;squashing&quot;: tf.nn.sigmoid
    }
    RESTORE_KEY = &quot;to_restore&quot;

    def __init__(self, architecture, d_hyperparams={}, meta_graph=None,
                 save_graph_def=True, log_dir=&quot;./log&quot;):
        &quot;&quot;&quot;(Re)build a symmetric VAE model with given:

         * architecture (list of nodes per encoder layer); e.g.
           [1000, 500, 250, 10] specifies a VAE with 1000-D inputs, 10-D latents,
           &amp;amp; end-to-end architecture [1000, 500, 250, 10, 250, 500, 1000]

         * hyperparameters (optional dictionary of updates to `DEFAULTS`)
        &quot;&quot;&quot;
        self.architecture = architecture
        self.__dict__.update(VAE.DEFAULTS, **d_hyperparams)
        self.sesh = tf.Session()

        if not meta_graph: # new model
            handles = self._buildGraph()
            ...
            self.sesh.run(tf.initialize_all_variables())
</code></pre><!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>    # encoding / &quot;recognition&quot;: q(z|x)
    encoding = [Dense(&quot;encoding&quot;, hidden_size, dropout, self.nonlinearity)
                # hidden layers reversed for function composition: outer -&amp;gt; inner
                for hidden_size in reversed(self.architecture[1:-1])]
    h_encoded = composeAll(encoding)(x_in)

    # latent distribution parameterized by hidden encoding
    # z ~ N(z_mean, np.exp(z_log_sigma)**2)
    z_mean = Dense(&quot;z_mean&quot;, self.architecture[-1], dropout)(h_encoded)
    z_log_sigma = Dense(&quot;z_log_sigma&quot;, self.architecture[-1], dropout)(h_encoded)
</code></pre>
<pre><code>
&lt;p&gt;Here, we build a pipe from &lt;code&gt;x_in&lt;/code&gt; (an empty placeholder for input data &lt;span class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt;), through the sequential hidden encoding, to the corresponding distribution over latent space—the variational approximate posterior, or hidden representation, &lt;span class=&quot;math inline&quot;&gt;\(z \sim q_\phi(z|x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As observed in lines &lt;code&gt;14&lt;/code&gt; - &lt;code&gt;15&lt;/code&gt;, latent &lt;span class=&quot;math inline&quot;&gt;\(z\)&lt;/span&gt; is distributed as a multivariate &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2465539/figure/fig1/&quot;&gt;normal&lt;/a&gt; with mean &lt;span class=&quot;math inline&quot;&gt;\(\mu\)&lt;/span&gt; and diagonal covariance values &lt;span class=&quot;math inline&quot;&gt;\(\sigma^2\)&lt;/span&gt; (the square of the “sigma” in &lt;code&gt;z_log_sigma&lt;/code&gt;) directly parameterized by the encoder: &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{N}(\mu, \sigma^2I)\)&lt;/span&gt;. In other words, we set out to “explain” highly complex observations as the consequence of an unobserved collection of simplified latent variables, i.e. independent Gaussians. (This is dictated by our choice of a conjugate spherical Gaussian prior over &lt;span class=&quot;math inline&quot;&gt;\(z\)&lt;/span&gt;—see &lt;a href=&quot;http://blog.fastforwardlabs.com/2016/08/12/introducing-variational-autoencoders-in-prose-and.html&quot;&gt;Part I&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;Next, we sample from this latent distribution (in practice, &lt;a href=&quot;https://arxiv.org/abs/1312.6114&quot;&gt;one draw is enough&lt;/a&gt; given sufficient minibatch size, i.e. &amp;gt;100). This method involves a trick—can you figure out why?—that we will explore in more detail later.&lt;/p&gt;
```python
        z = self.sampleGaussian(z_mean, z_log_sigma)
</code></pre><!-- raw HTML omitted -->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">        <span style="color:#75715e"># decoding / &#34;generative&#34;: p(x|z)</span>
        decoding <span style="color:#f92672">=</span> [Dense(<span style="color:#e6db74">&#34;decoding&#34;</span>, hidden_size, dropout, self<span style="color:#f92672">.</span>nonlinearity)
                    <span style="color:#66d9ef">for</span> hidden_size <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>architecture[<span style="color:#ae81ff">1</span>:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]] <span style="color:#75715e"># assumes symmetry</span>
        <span style="color:#75715e"># final reconstruction: restore original dims, squash outputs [0, 1]</span>
        decoding<span style="color:#f92672">.</span>insert(<span style="color:#ae81ff">0</span>, Dense( <span style="color:#75715e"># prepend as outermost function</span>
            <span style="color:#e6db74">&#34;reconstruction&#34;</span>, self<span style="color:#f92672">.</span>architecture[<span style="color:#ae81ff">0</span>], dropout, self<span style="color:#f92672">.</span>squashing))
        x_reconstructed <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>identity(composeAll(decoding)(z), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;x_reconstructed&#34;</span>)
</code></pre></div><!-- raw HTML omitted -->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">        <span style="color:#75715e"># ops to directly explore latent space</span>
        <span style="color:#75715e"># defaults to prior z ~ N(0, I)</span>
        z_ <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>placeholder_with_default(tf<span style="color:#f92672">.</span>random_normal([<span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>architecture[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]]),
                                         shape<span style="color:#f92672">=</span>[None, self<span style="color:#f92672">.</span>architecture[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]],
                                         name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;latent_in&#34;</span>)
        x_reconstructed_ <span style="color:#f92672">=</span> composeAll(decoding)(z_)
</code></pre></div><!-- raw HTML omitted -->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sampleGaussian</span>(self, mu, log_sigma):
        <span style="color:#e6db74">&#34;&#34;&#34;Draw sample from Gaussian with given shape, subject to random noise epsilon&#34;&#34;&#34;</span>
        <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>name_scope(<span style="color:#e6db74">&#34;sample_gaussian&#34;</span>):
            <span style="color:#75715e"># reparameterization trick</span>
            epsilon <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>random_normal(tf<span style="color:#f92672">.</span>shape(log_sigma), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;epsilon&#34;</span>)
            <span style="color:#66d9ef">return</span> mu <span style="color:#f92672">+</span> epsilon <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>exp(log_sigma) <span style="color:#75715e"># N(mu, sigma**2)</span>
</code></pre></div><!-- raw HTML omitted -->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#a6e22e">@staticmethod</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">crossEntropy</span>(obs, actual, offset<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-7</span>):
        <span style="color:#e6db74">&#34;&#34;&#34;Binary cross-entropy, per training example&#34;&#34;&#34;</span>
        <span style="color:#75715e"># (tf.Tensor, tf.Tensor, float) -&amp;gt; tf.Tensor</span>
        <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>name_scope(<span style="color:#e6db74">&#34;cross_entropy&#34;</span>):
            <span style="color:#75715e"># bound by clipping to avoid nan</span>
            obs_ <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>clip_by_value(obs, offset, <span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> offset)
            <span style="color:#66d9ef">return</span> <span style="color:#f92672">-</span>tf<span style="color:#f92672">.</span>reduce_sum(actual <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>log(obs_) <span style="color:#f92672">+</span>
                                  (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> actual) <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> obs_), <span style="color:#ae81ff">1</span>)
</code></pre></div><!-- raw HTML omitted -->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#a6e22e">@staticmethod</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">kullbackLeibler</span>(mu, log_sigma):
        <span style="color:#e6db74">&#34;&#34;&#34;(Gaussian) Kullback-Leibler divergence KL(q||p), per training example&#34;&#34;&#34;</span>
        <span style="color:#75715e"># (tf.Tensor, tf.Tensor) -&amp;gt; tf.Tensor</span>
        <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>name_scope(<span style="color:#e6db74">&#34;KL_divergence&#34;</span>):
            <span style="color:#75715e"># = -0.5 * (1 + log(sigma**2) - mu**2 - sigma**2)</span>
            <span style="color:#66d9ef">return</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">0.5</span> <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>reduce_sum(<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> log_sigma <span style="color:#f92672">-</span> mu<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span> <span style="color:#f92672">-</span>
                                        tf<span style="color:#f92672">.</span>exp(<span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> log_sigma), <span style="color:#ae81ff">1</span>)
</code></pre></div><!-- raw HTML omitted -->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">        <span style="color:#75715e"># reconstruction loss: mismatch b/w x &amp;amp; x_reconstructed</span>
        <span style="color:#75715e"># binary cross-entropy -- assumes p(x) &amp;amp; p(x|z) are iid Bernoullis</span>
        rec_loss <span style="color:#f92672">=</span> VAE<span style="color:#f92672">.</span>crossEntropy(x_reconstructed, x_in)

        <span style="color:#75715e"># Kullback-Leibler divergence: mismatch b/w approximate posterior &amp;amp; imposed prior</span>
        <span style="color:#75715e"># KL[q(z|x) || p(z)]</span>
        kl_loss <span style="color:#f92672">=</span> VAE<span style="color:#f92672">.</span>kullbackLeibler(z_mean, z_log_sigma)

        <span style="color:#75715e"># average over minibatch</span>
        cost <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(rec_loss <span style="color:#f92672">+</span> kl_loss, name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;cost&#34;</span>)
</code></pre></div><!-- raw HTML omitted -->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">        <span style="color:#75715e"># optimization</span>
        global_step <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(<span style="color:#ae81ff">0</span>, trainable<span style="color:#f92672">=</span>False)
        <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>name_scope(<span style="color:#e6db74">&#34;Adam_optimizer&#34;</span>):
            optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>train<span style="color:#f92672">.</span>AdamOptimizer(self<span style="color:#f92672">.</span>learning_rate)
            tvars <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>trainable_variables()
            grads_and_vars <span style="color:#f92672">=</span> optimizer<span style="color:#f92672">.</span>compute_gradients(cost, tvars)
            clipped <span style="color:#f92672">=</span> [(tf<span style="color:#f92672">.</span>clip_by_value(grad, <span style="color:#f92672">-</span><span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>), tvar) <span style="color:#75715e"># gradient clipping</span>
                    <span style="color:#66d9ef">for</span> grad, tvar <span style="color:#f92672">in</span> grads_and_vars]
            train_op <span style="color:#f92672">=</span> optimizer<span style="color:#f92672">.</span>apply_gradients(clipped, global_step<span style="color:#f92672">=</span>global_step,
                                                 name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;minimize_cost&#34;</span>) <span style="color:#75715e"># back-prop</span>
</code></pre></div><!-- raw HTML omitted -->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">        <span style="color:#66d9ef">return</span> (x_in, dropout, z_mean, z_log_sigma, x_reconstructed,
                z_, x_reconstructed_, cost, global_step, train_op)
</code></pre></div><!-- raw HTML omitted -->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train</span>(self, X, max_iter<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>inf, max_epochs<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>inf, cross_validate<span style="color:#f92672">=</span>True,
              verbose<span style="color:#f92672">=</span>True, save<span style="color:#f92672">=</span>False, outdir<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;./out&#34;</span>, plots_outdir<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;./png&#34;</span>):
        <span style="color:#66d9ef">try</span>:
            err_train <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
            now <span style="color:#f92672">=</span> datetime<span style="color:#f92672">.</span>now()<span style="color:#f92672">.</span>isoformat()[<span style="color:#ae81ff">11</span>:]
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;------- Training begin: {} -------</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(now))

            <span style="color:#66d9ef">while</span> True:
                x, _ <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>train<span style="color:#f92672">.</span>next_batch(self<span style="color:#f92672">.</span>batch_size)
                feed_dict <span style="color:#f92672">=</span> {self<span style="color:#f92672">.</span>x_in: x, self<span style="color:#f92672">.</span>dropout_: self<span style="color:#f92672">.</span>dropout}
                fetches <span style="color:#f92672">=</span> [self<span style="color:#f92672">.</span>x_reconstructed, self<span style="color:#f92672">.</span>cost, self<span style="color:#f92672">.</span>global_step, self<span style="color:#f92672">.</span>train_op]
                x_reconstructed, cost, i, _ <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>sesh<span style="color:#f92672">.</span>run(fetches, feed_dict)

                err_train <span style="color:#f92672">+=</span> cost

                <span style="color:#66d9ef">if</span> i<span style="color:#f92672">%</span><span style="color:#ae81ff">1000</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">and</span> verbose:
                    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;round {} --&amp;gt; avg cost: &#34;</span><span style="color:#f92672">.</span>format(i), err_train <span style="color:#f92672">/</span> i)

                <span style="color:#66d9ef">if</span> i <span style="color:#f92672">&amp;</span>gt;<span style="color:#f92672">=</span> max_iter <span style="color:#f92672">or</span> X<span style="color:#f92672">.</span>train<span style="color:#f92672">.</span>epochs_completed <span style="color:#f92672">&amp;</span>gt;<span style="color:#f92672">=</span> max_epochs:
                    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;final avg cost (@ step {} = epoch {}): {}&#34;</span><span style="color:#f92672">.</span>format(
                        i, X<span style="color:#f92672">.</span>train<span style="color:#f92672">.</span>epochs_completed, err_train <span style="color:#f92672">/</span> i))
                    now <span style="color:#f92672">=</span> datetime<span style="color:#f92672">.</span>now()<span style="color:#f92672">.</span>isoformat()[<span style="color:#ae81ff">11</span>:]
                    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;------- Training end: {} -------</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(now))
                    <span style="color:#66d9ef">break</span>
</code></pre></div><!-- raw HTML omitted -->
    <div class="spacer"></div>
    <div>
      <div style="width: 100%; height: 2px; background: #ccc; margin-top: -1px; margin-bottom: -1px;"></div>
    </div>
    <div class="spacer"></div>
  </div>
</div>


<div class="container">
  <div class="spacer"></div>
  <h2 class="clear">Read more</h2>
  <div class="spacer"></div>
  <div style="display: grid; grid-template-columns: 1fr 1fr; grid-column-gap: 2ch;">
    <div>
      
      <div class="small">Newer</div>
      <div>
  <h5 style="margin-bottom: 0px;">
    <span>Aug 24, 2016</span> &middot;
    <span style="text-transform: capitalize;">
      interview
    </span>
  </h5>
  
    <div><a href="/2016/08/24/next-economics-interview-with-jimi-crawford.html"><strong>Next Economics: Interview with Jimi Crawford</strong></a></div>
  
  <div class="spacer"></div>
</div>


      
    </div>
    <div>
      
      <div class="small">Older</div>
      <div>
  <h5 style="margin-bottom: 0px;">
    <span>Aug 18, 2016</span> &middot;
    <span style="text-transform: capitalize;">
      guest post
    </span>
  </h5>
  
    <div><a href="/2016/08/18/giving-speech-a-voice-in-the-home.html"><strong>Giving Speech a Voice in the Home</strong></a></div>
  
  <div class="spacer"></div>
</div>


      
    </div>
  </div>
</div>

<div class="container">
<div class="spacer"></div>
<div>
  <div style="width: 100%; height: 2px; background: #ccc; margin-top: -1px; margin-bottom: -1px;"></div>
</div>
<div class="spacer"></div>
<div class="spacer"></div>
</div>

<div class="container">
  

<h2 class="clear">Latest posts</h2>
<div class="spacer"></div>

<div id="posts-holder"> 
  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Jun 22, 2020</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
  <a href="/2020/06/22/how-to-explain-huggingface-bert-for-question-answering-nlp-models-with-tf-2.0.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/explanation-1592852095.jpg" />
  </a>
  
  <div>
    
    <a href="/2020/06/22/how-to-explain-huggingface-bert-for-question-answering-nlp-models-with-tf-2.0.html"
       ><h2 style="margin-bottom: 4px;">How to Explain HuggingFace BERT for Question Answering NLP Models with TF 2.0</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by 
      <span
        ><a href="https://twitter.com/vykthur">Victor</a>
        &middot; </span
      >
    </span>
    In this sample, a BERTbase model gets the answer correct (Achaemenid Persia). Model gradients show that the token &ldquo;subordinate ..&rdquo; is impactful in the selection of an answer to the question &ldquo;Macedonia was under the rule of which country?&quot;. This makes sense .. good for BERTbase. Recently, our team at Fast Forward Labs have been exploring state of the art models for Question Answering and have used the rather excellent HuggingFace transformers library.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/2020/06/22/how-to-explain-huggingface-bert-for-question-answering-nlp-models-with-tf-2.0.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Jun 16, 2020</span> &middot;
    <span style="text-transform: capitalize;">
      notebook
    </span>
  </h5>
  
  <a href="/2020/06/16/evaluating-qa-metrics-predictions-and-the-null-response.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/shotwin-2020-06-16_09-31-48-1592314597.png" />
  </a>
  
  <div>
    
    <a href="https://qa.fastforwardlabs.com/no%20answer/null%20threshold/bert/distilbert/exact%20match/f1/robust%20predictions/2020/06/09/Evaluating_BERT_on_SQuAD.html" target="_blank">
      <h2 style="margin-bottom: 4px;">Evaluating QA: Metrics, Predictions, and the Null Response →</h2></a
    >
    
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by 
      <span
        ><a href="https://www.linkedin.com/in/melanierbeck">Melanie</a>
        &middot; </span
      >
    </span>
    A deep dive into computing QA predictions and when to tell BERT to zip it! In our last post, Building a QA System with BERT on Wikipedia, we used the HuggingFace framework to train BERT on the SQuAD2.0 dataset and built a simple QA system on top of the Wikipedia search engine. This time, we&rsquo;ll look at how to assess the quality of a BERT-like model for Question Answering.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="https://qa.fastforwardlabs.com/no%20answer/null%20threshold/bert/distilbert/exact%20match/f1/robust%20predictions/2020/06/09/Evaluating_BERT_on_SQuAD.html" target="_blank">
        	
        qa.fastforwardlabs.com
      </a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>May 19, 2020</span> &middot;
    <span style="text-transform: capitalize;">
      notebook
    </span>
  </h5>
  
  <a href="/2020/05/19/building-a-qa-system-with-bert-on-wikipedia.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/markus-spiske-C0koz3G1I4I-unsplash.jpg" />
  </a>
  
  <div>
    
    <a href="https://qa.fastforwardlabs.com/pytorch/hugging%20face/wikipedia/bert/transformers/2020/05/19/Getting_Started_with_QA.html" target="_blank">
      <h2 style="margin-bottom: 4px;">Building a QA System with BERT on Wikipedia →</h2></a
    >
    
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by 
      <span
        ><a href="https://www.linkedin.com/in/melanierbeck">Melanie</a>
        &middot; </span
      >
    </span>
    So you&rsquo;ve decided to build a QA system. You want to start with something simple and general so you plan to make it open domain using Wikipedia as a corpus for answering questions. You want to use the best NLP that your compute resources allow (you&rsquo;re lucky enough to have access to a GPU) so you&rsquo;re going to focus on the big, flashy Transformer models that are all the rage these days.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="https://qa.fastforwardlabs.com/pytorch/hugging%20face/wikipedia/bert/transformers/2020/05/19/Getting_Started_with_QA.html" target="_blank">
        	
        qa.fastforwardlabs.com
      </a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Apr 28, 2020</span> &middot;
    <span style="text-transform: capitalize;">
      notebook
    </span>
  </h5>
  
  <a href="/2020/04/28/intro-to-automated-question-answering.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/qa-workflow.png" />
  </a>
  
  <div>
    
    <a href="https://qa.fastforwardlabs.com/methods/background/2020/04/28/Intro-to-QA.html" target="_blank">
      <h2 style="margin-bottom: 4px;">Intro to Automated Question Answering →</h2></a
    >
    
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by 
      <span
        ><a href="https://www.linkedin.com/in/melanierbeck">Melanie</a>
        &middot; </span
      >
    </span>
    Welcome to the first edition of the Cloudera Fast Forward blog on Natural Language Processing for Question Answering! Throughout this series, we’ll build a Question Answering (QA) system with off-the-shelf algorithms and libraries and blog about our process and what we find along the way. We hope to wind up with a beginning-to-end documentary that provides:
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="https://qa.fastforwardlabs.com/methods/background/2020/04/28/Intro-to-QA.html" target="_blank">
        	
        qa.fastforwardlabs.com
      </a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Apr 1, 2020</span> &middot;
    <span style="text-transform: capitalize;">
      newsletter
    </span>
  </h5>
  
  <a href="/2020/04/01/enterprise-grade-ml.html" class="preview-image-holder">
    <img class="preview-image" src="/images/2020/03/Screen_Shot_2020_03_27_at_4_17_39_PM-1585340376058.png" />
  </a>
  
  <div>
    
    <a href="/2020/04/01/enterprise-grade-ml.html"
       ><h2 style="margin-bottom: 4px;">Enterprise Grade ML</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by 
      <span
        ><a href="https://twitter.com/shioulin_sam">Shioulin</a>
        &middot; </span
      >
    </span>
    At Cloudera Fast Forward, one of the mechanisms we use to tightly couple machine learning research with application is through application development projects for both internal and external clients. The problems we tackle in these projects are wide ranging and cut across various industries; the end goal is a production system that translates data into business impact.
What is Enterprise Grade Machine Learning? Enterprise grade ML, a term mentioned in a paper put forth by Microsoft, refers to ML applications where there is a high level of scrutiny for data handling, model fairness, user privacy, and debuggability.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/2020/04/01/enterprise-grade-ml.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Apr 1, 2020</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
  <a href="/2020/04/01/bias-in-knowledge-graphs-part-1.html" class="preview-image-holder">
    <img class="preview-image" src="/images/editor_uploads/2020-03-28-150645-balance_2108024_1920.jpg" />
  </a>
  
  <div>
    
    <a href="/2020/04/01/bias-in-knowledge-graphs-part-1.html"
       ><h2 style="margin-bottom: 4px;">Bias in Knowledge Graphs - Part 1</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by 
      <span
        ><a href="https://twitter.com/keitabr">Keita</a>
        &middot; </span
      >
    </span>
    Introduction This is the first part of a series to review Bias in Knowledge Graphs (KG). We aim to describe methods of identifying bias, measuring its impact, and mitigating that impact. For this part, we’ll give a broad overview of this topic.
image credit: Mediamodifier from Pixabay Motivation Knowledge graphs, graphs with built-in ontologies, create unique opportunities for data analytics, machine learning, and data mining. They do this by enhancing data with the power of connections and human knowledge.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/2020/04/01/bias-in-knowledge-graphs-part-1.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
</div>

<div>
  <button id="load_more" style="width: 100%;">load more</button>
</div>
<div class="spacer"></div>
<div class="spacer"></div>

<script>
  window.addEventListener('load', () => {
    let $posts_holder = document.getElementById('posts-holder')
    let $load_more = document.getElementById('load_more')
    let next_page = 2
    $load_more.addEventListener('click', () => {
      fetch(`/posts/page/${next_page}.html`).then(r =>r.text()).then(r => {
        let el = document.createElement('html')
        el.innerHTML = r
        next_page += 1
        let $posts = el.querySelector('#posts-holder').children
        for (let i=0; i< $posts.length; i++) {
          let $post = $posts[i].cloneNode(true)
          $posts_holder.appendChild($post)
        }
      })
    })
  })
</script>


  <h3 class="clear">Popular posts</h3>
<div class="spacer"></div>
<div>
  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Oct 30, 2019</span> &middot;
    <span style="text-transform: capitalize;">
      newsletter
    </span>
  </h5>
  
    <div><a href="/2019/10/30/exciting-applications-of-graph-neural-networks.html"><strong>Exciting Applications of Graph Neural Networks</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Nov 14, 2018</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2018/11/14/federated-learning-distributed-machine-learning-with-data-locality-and-privacy.html"><strong>Federated learning: distributed machine learning with data locality and privacy</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Apr 10, 2018</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2018/04/10/pytorch-for-recommenders-101.html"><strong>PyTorch for Recommenders 101</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Oct 4, 2017</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2017/10/04/first-look-using-three.js-for-2d-data-visualization.html"><strong>First Look: Using Three.js for 2D Data Visualization</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Aug 22, 2016</span> &middot;
    <span style="text-transform: capitalize;">
      whitepaper
    </span>
  </h5>
  
    <div><a href="/2016/08/22/under-the-hood-of-the-variational-autoencoder-in-prose-and-code.html"><strong>Under the Hood of the Variational Autoencoder (in Prose and Code)</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Feb 24, 2016</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2016/02/24/hello-world-in-keras-or-scikit-learn-versus-keras.html"><strong>&#34;Hello world&#34; in Keras (or, Scikit-learn versus Keras)</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
</div>

</div>

<div class="spacer"></div>
<div style="background: #efefef;">
  <div class="spacer"></div>
  <div class="spacer"></div>
  <div class="container">
  <h1 class="clear">Reports</h1>
  <div style="color: #444;">In-depth guides to specific machine learning capabilities</div>
</div>
<div class="spacer"></div>
<div style="max-width: 96ch; margin: 0 auto; padding-left: 1ch; padding-right: 1ch;">
  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF13</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://ff13.fastforwardlabs.com" target="_blank">Causality for Maching Learning</a></h2>
  <a class="report-image" href="https://ff13.fastforwardlabs.com" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff13-combo.png" />
  </a>
  <div class="small">The intersection of causal inference and machine learning is a rapidly expanding area of research that&#39;s already yielding capabilities to enable building more robust, reliable, and fair machine learning systems. This report offers an introduction to causal reasoning including causal graphs and invariant prediction and how to apply causal inference tools together with classic machine learning techniques in multiple use-cases.</div>
  <div><a href="https://ff13.fastforwardlabs.com" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF06-2020</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://ff06-2020.fastforwardlabs.com" target="_blank">Interpretability</a></h2>
  <a class="report-image" href="https://ff06-2020.fastforwardlabs.com" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff06-2020-combo.png" />
  </a>
  <div class="small">Interpretability, or the ability to explain why and how a system makes a decision, can help us improve models, satisfy regulations, and build better products. Black-box techniques like deep learning have delivered breakthrough capabilities at the cost of interpretability. In this report, recently updated to include techniques like SHAP, we show how to make models interpretable without sacrificing their capabilities or accuracy.</div>
  <div><a href="https://ff06-2020.fastforwardlabs.com" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF12</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://ff12.fastforwardlabs.com" target="_blank">Deep Learning for Anomaly Detection</a></h2>
  <a class="report-image" href="https://ff12.fastforwardlabs.com" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff12-combo.png" />
  </a>
  <div class="small">From fraud detection to flagging abnormalities in imaging data, there are countless applications for automatic identification of abnormal data. This process can be challenging, especially when working with large, complex data. This report explores deep learning approaches (sequence models, VAEs, GANs) for anomaly detection, when to use them, performance benchmarks, and product possibilities.</div>
  <div><a href="https://ff12.fastforwardlabs.com" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
</div>


</div>

<div class="spacer"></div>
<div class="spacer"></div>
 
<div class="container">
  <h1 class="clear">Prototypes</h1>
  <div style="color: #444;">Machine learning prototypes and interactive notebooks</div>
  <div class="spacer"></div>
</div>
<div id="prototypes-holder">
  
  <div style="display: flex; flex-direction: column; position: relative; padding-left: 1ch; padding-right: 1ch; height: 360px;">
  <div>
    <h5 style="margin-bottom: 0">Notebooks</h5>
    <h2 style="padding-top: 0; margin-bottom: 0;"><a href="https://qa.fastforwardlabs.com" target="_blank">NLP for Question Answering</a></h2>
  </div>
  <div style="flex: 1 1 auto; position: relative;">
    <a href="https://qa.fastforwardlabs.com" target="_blank" style="display: block; position: absolute; top: 0.375rem; width: 100%; height: calc(100% - 0.875rem); background-image: url('https://experiments.fastforwardlabs.com/images/uploads/qa.png'); background-size: contain; background-position: center left; background-repeat: no-repeat;"></a>
  </div>
  <div>
    <div class="small">Ongoing posts and code documenting the process of building a question answering model.</div>
    <div style="width: 100%; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"><a href="https://qa.fastforwardlabs.com" target="_blank">https://qa.fastforwardlabs.com</a></div>
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
  <div style="display: flex; flex-direction: column; position: relative; padding-left: 1ch; padding-right: 1ch; height: 360px;">
  <div>
    <h5 style="margin-bottom: 0">Notebook</h5>
    <h2 style="padding-top: 0; margin-bottom: 0;"><a href="https://colab.research.google.com/drive/1pjPzsw_uZew-Zcz646JTkRDhF2GkPk0N" target="_blank">Interpretability Revisited: SHAP and LIME</a></h2>
  </div>
  <div style="flex: 1 1 auto; position: relative;">
    <a href="https://colab.research.google.com/drive/1pjPzsw_uZew-Zcz646JTkRDhF2GkPk0N" target="_blank" style="display: block; position: absolute; top: 0.375rem; width: 100%; height: calc(100% - 0.875rem); background-image: url('https://experiments.fastforwardlabs.com/images/uploads/shap-and-lime.png'); background-size: contain; background-position: center left; background-repeat: no-repeat;"></a>
  </div>
  <div>
    <div class="small">Explore how to use LIME and SHAP for interpretability.</div>
    <div style="width: 100%; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"><a href="https://colab.research.google.com/drive/1pjPzsw_uZew-Zcz646JTkRDhF2GkPk0N" target="_blank">https://colab.research.google.com/drive/1pjPzsw_uZew-Zcz646JTkRDhF2GkPk0N</a></div>
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
  <div style="display: flex; flex-direction: column; position: relative; padding-left: 1ch; padding-right: 1ch; height: 360px;">
  <div>
    <h5 style="margin-bottom: 0">Prototype</h5>
    <h2 style="padding-top: 0; margin-bottom: 0;"><a href="https://refractor.fastforwardlabs.com" target="_blank">Refractor</a></h2>
  </div>
  <div style="flex: 1 1 auto; position: relative;">
    <a href="https://refractor.fastforwardlabs.com" target="_blank" style="display: block; position: absolute; top: 0.375rem; width: 100%; height: calc(100% - 0.875rem); background-image: url('https://experiments.fastforwardlabs.com/images/uploads/refractor.gif'); background-size: contain; background-position: center left; background-repeat: no-repeat;"></a>
  </div>
  <div>
    <div class="small">Refractor predicts churn probabilities for telecom customers and shows which customer attributes contribute to those predictions.</div>
    <div style="width: 100%; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"><a href="https://refractor.fastforwardlabs.com" target="_blank">https://refractor.fastforwardlabs.com</a></div>
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
  <div style="display: flex; flex-direction: column; position: relative; padding-left: 1ch; padding-right: 1ch; height: 360px;">
  <div>
    <h5 style="margin-bottom: 0">Prototype</h5>
    <h2 style="padding-top: 0; margin-bottom: 0;"><a href="https://anomagram.fastforwardlabs.com" target="_blank">Anomagram</a></h2>
  </div>
  <div style="flex: 1 1 auto; position: relative;">
    <a href="https://anomagram.fastforwardlabs.com" target="_blank" style="display: block; position: absolute; top: 0.375rem; width: 100%; height: calc(100% - 0.875rem); background-image: url('https://experiments.fastforwardlabs.com/images/uploads/anomagram.gif'); background-size: contain; background-position: center left; background-repeat: no-repeat;"></a>
  </div>
  <div>
    <div class="small">An interactive visualization tool for exploring how a deep learning model can be applied to the task of anomaly detection.</div>
    <div style="width: 100%; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"><a href="https://anomagram.fastforwardlabs.com" target="_blank">https://anomagram.fastforwardlabs.com</a></div>
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
</div>
<div class="container">
  <div ><button id="load_all_prototypes" style="width: 100%;">load all</button></div>
</div>

<script>
  
  window.addEventListener('load', () => {
    let $prototypes_holder = document.getElementById('prototypes-holder')
    let $load_more = document.getElementById('load_all_prototypes')
    $load_more.addEventListener('click', () => {
      fetch(`/prototypes.html`).then(r =>r.text()).then(r => {
        $load_more.remove()
        let el = document.createElement('html')
        el.innerHTML = r
        let $posts = el.querySelector('#prototypes-holder')
        $prototypes_holder.innerHTML = $posts.innerHTML
      })
    })
  })
</script>



<div class="spacer"></div>
<div class="spacer"></div>

<div class="container">
  <div>
    <h1 class="clear">About</h1>
    <div>
      Cloudera Fast Forward is an applied machine learning reseach group.<br />
      <a
        href="https://www.cloudera.com/products/fast-forward-labs-research.html"
        >Cloudera</a
      >&nbsp;&nbsp;
      <a
        href="https://blog.fastforwardlabs.com"
        >Blog</a
      >&nbsp;&nbsp;
      <a href="https://twitter.com/fastforwardlabs">Twitter</a>
    </div>
  </div>
</div>



<div class="spacer"></div>
<div class="spacer"></div>


      </main>
 </body>
</html>
