---
date: 2020-07-23T12:28:14Z
---

Welcome to the July edition of Cloudera Fast Forward's monthly newsletter. This month, we have new research, new blog posts, new recommended reading, and we're very excited to invite you to our next research webinar _tomorrow_ -- [Deep Learning for Automated Question Answering](https://www.cloudera.com/about/events/webinars/deep-learning-for-automated-question-answering.html?internal_keyplay=ODL&internal_campaign=FY21-Q2_CW_AMER_CFFL_14_Report_EP_2020-07-30&cid=7012H000000sW8u&internal_link=p07)!

---

## New research: NLP for Question Answering

We're rounding out our blog series on Question Answering with these three recent posts.

[Evaluating QA: the Retriever & the Full QA System](https://qa.fastforwardlabs.com/elasticsearch/mean%20average%20precision/recall%20for%20irqa/qa%20system%20design/2020/06/30/Evaluating_the_Retriever_&_End_to_End_System.html)

This post focuses on a vital component of a modern Information Retrieval-based (IR) QA system: the Retriever. Specifically, we introduce Elasticsearch as a powerful and efficient IR tool that can be used to scour through large corpora and retrieve relevant documents. We explain how to implement and evaluate a Retriever in the context of Question Answering and demonstrate its impact on an IR QA system.

[How to Maximize Retriever Performance on a More Natural Dataset](https://qa.fastforwardlabs.com/elasticsearch/qa%20system%20design/passage%20ranking/masked%20language%20model/word%20embeddings/2020/07/22/Improving_the_Retriever_on_Natural_Questions.html)

Implementing question answering for real-world use cases is a bit more nuanced than evaluating system performance against a toy dataset. In this post, we explore several challenges faced by the Retriever when applying IR-QA to a more realistic dataset, as well as a few practical approaches for overcoming them.

[Beyond SQuAD: How to Apply a Transformer QA Model to _Your_ Data](https://qa.fastforwardlabs.com/domain%20adaptation/transfer%20learning/specialized%20datasets/qa/medical%20qa/2020/07/22/QA-for-Specialized-Data.html)

Finally, we discuss how a Reader trained on SQuAD2.0 might perform on different datasets, particularly on highly specialized data — such as a collection of legal contracts, financial reports, or technical manuals. In this post we perform experiments designed to highlight how to adapt Transformer models to specialized domains and provide guidelines for practical applications.

If you're just catching up to our Question Answering series, check out the first three posts:

[Intro to Automated Question Answering](https://qa.fastforwardlabs.com/methods/background/2020/04/28/Intro-to-QA.html)

[Building a QA System with BERT on Wikipedia](https://qa.fastforwardlabs.com/pytorch/hugging%20face/wikipedia/bert/transformers/2020/05/19/Getting_Started_with_QA.html)

[Evaluating QA: Metrics, Predictions, and the Null Response](https://qa.fastforwardlabs.com/no%20answer/null%20threshold/bert/distilbert/exact%20match/f1/robust%20predictions/2020/06/09/Evaluating_BERT_on_SQuAD.html)

---

## New on the blog

[How to Explain HuggingFace BERT for Question Answering NLP Models with TF 2.0](https://blog.fastforwardlabs.com/2020/06/22/how-to-explain-huggingface-bert-for-question-answering-nlp-models-with-tf-2.0.html)

![How to Explain HuggingFace BERT for Question Answering NLP Models with TF 2.0](/images/hugo/explanation-1592852095.jpg)

Recently, our team at Fast Forward Labs have been exploring state of the art models for [Question Answering](https://qa.fastforwardlabs.com/) and have used the rather excellent HuggingFace [transformers](https://github.com/huggingface/transformers/) library. As we applied BERT for QA models (BERTQA) to datasets outside of wikipedia (e.g legal documents), we have observed a variety of results. Naturally, one of the things we have been exploring are methods to better understand why the model provides certain responses, and especially when it fails. This post focuses on the following questions:

- What are some approaches for explaining a BERT based model?
- Why are Gradients a good approach?
- How to implement Gradient explanations for BERT in Tensorflow 2.0?
- Some example results and visualizations!

The post comes along with an [interactive Colab notebook](https://colab.research.google.com/drive/1tTiOgJ7xvy3sjfiFC9OozbjAX1ho8WN9?usp=sharing) which you can try out! - [Victor](https://twitter.com/vykthur)

---

## Recommended reading

Our research engineers share their favourite reads of the month.

- [Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead](https://arxiv.org/abs/1811.10154)
  I recall this paper making a splash when it appeared in late 2018. I don't recall why I didn't read it at the time, and more fool me: it is a tour-de-force of constructive criticism. Cynthia Rudin draws a clear distinction between explainable and interpretable machine learning, and enumerates the challenges with each, and why we should prefer the latter. This is a must-read for anyone working on use cases that benefit from interpretability. - [Chris](https://twitter.com/_cjwallace)
- [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/pdf/2004.04906.pdf)

  As we’ve learned through the FF14 blog series, open-domain question answering relies on efficient passage retrieval to first narrow large corpora of text into a handful of context passages for answer extraction. While traditional methods for passage retrieval rely on sparse vector models, this paper demonstrates how learned, dense representations for retrieval can help establish state-of-the-art performance on several end-to-end QA benchmarks. - [Andrew](https://www.linkedin.com/in/andrew-r-reed/)

- [On Lacework by Everest Pipkin](https://unthinking.photography/projects/lacework/index_2.html)
  An artistic examination of a machine learning dataset and the assumptions and invisible labor that go into it. - [Grant](https://feed.grantcuster.com)
- [Snorkel AI: Putting Data First in ML Development](https://www.snorkel.ai/07-14-2020-snorkel-ai-launch) - We are excited to see this project that started in 2016 launch out of stealth! Snorkel AI enables machine learning practitioners to automate the data labeling process using an approach called weak supervision. Almost a year back we had a chance to tinker around with the library and also published a few [blog posts](https://blog.fastforwardlabs.com/2019/07/08/taking-snorkel-for-a-spin.html). Things have much changed since then. The library has now evolved into Snorkel Flow - an end-to-end process of developing and deploying ML-powered applications by iteratively and programmatically labeling, augmenting, and building training data. - [Nisha](https://twitter.com/NishaMuktewar)
- [Tempering Expectations for GPT-3 and OpenAI’s API](https://minimaxir.com/2020/07/gpt3-expectations/)
  OpenAI recently released their GPT-3 API to beta users, and the accompanying paper [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165). Reactions range from bored scepticism to skynet prophesying. This post from Max Woolf gives an informed take on what is a significant algorithmic and engineering advancement, but not magic. - [Chris](https://twitter.com/_cjwallace)
- [Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping](https://arxiv.org/abs/2002.06305)
  A must-read for any NLP practitioner! The authors perform a deep study into how fine-tuning language models (like BERT and XLNet) is a very brittle process. They demonstrate that, even with a fixed set of hyperparameters, results vary substantially due to the value of the random seed and even by the order of the training examples during training! They then offer practical guidance on how and when to stop training less promising models in favor of those more likely to converge, thus saving time and, ultimately, producing better models. - [Melanie](https://www.linkedin.com/in/melanierbeck/)
- [Estimating Displaced Populations from Overhead](https://arxiv.org/abs/2006.14547) - This paper uses overhead drone imagery (sub-meter, 10cm Ground Sample Distance), population polygons, Open Street Map (OSM) segmentation masks and convolutional network to estimate refugee camp population. It's a great illustration of using ML and improved sensor capabilities for social good. - Shioulin

---

## Events

### Webinar: [Deep Learning for Automated Question Answering](https://www.cloudera.com/about/events/webinars/deep-learning-for-automated-question-answering.html?internal_keyplay=ODL&internal_campaign=FY21-Q2_CW_AMER_CFFL_14_Report_EP_2020-07-30&cid=7012H000000sW8u&internal_link=p07)

_July 30, 2020 10:00am PT_&nbsp; | _1:00pm ET_

What if you could ask your email client, “Who sent me the link with the latest financial report?” Automated question answering is a human-machine interaction to extract information from data using natural language. This general capability can take many forms, but one of the most exciting developments has been question answering from unstructured text data, including the massive amounts of information contained in emails, social media posts, blogs, log files, financial statements -- and the list goes on. Thanks to a series of advances in deep learning techniques in the past two years, question answering capabilities have grown rapidly, and while still emerging, it’s the perfect time to examine how this technology works, when it works well, and where it might still fall short.

In this webinar, we'll cover

- General architecture of modern QA systems
- Deep learning techniques for QA
- Guidance on applying these techniques to a practical use case

We’ll also do a live demonstration of a QA prototype that we built. You won’t want to miss it, so we hope to see you there!
