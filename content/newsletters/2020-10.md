---
date: 2020-10-23T21:15:01Z
---

Welcome to the October edition of Cloudera Fast Forward's monthly newsletter. We're happy to share some our latest research, and invite you to our next webinar: tomorrow!

---

## New research release!

### Structural Time Series

![Structural Time Series cover image](/images/hugo/ff16-cover-splash-1603487224.png)

Time series data is ubiquitous, and forecasting has a long history. Generalized additive models give us a simple, flexible and interpretable means for modeling some kinds of time series, especially where there is seasonality. We look at the benefits and trade-offs of taking a curve-fitting approach to time series, and demonstrate its use via Facebook's Prophet library on a demand forecasting problem.

Our report, [Structural Time Series](https://structural-time-series.fastforwardlabs.com/), is freely available online, and accompanied by [code](https://github.com/fastforwardlabs/structural-time-series/) applying the techniques discussed to forecasting electricity demand in California.

### Research preview: Semantic Image Search

Within this research cycle, we will be revisiting the topic of semantic search on image data. We explore two critical requirements for semantic search at scale - strategies for creating semantic representations of images (supervised, unsupervised, semi supervised methods) and methods for fast approximate nearest neighbor search  (e.g. FAISS). We will also be releasing an update to the well used [ConvNet Playground App](http://convnetplayground.fastforwardlabs.com/), and a set of scripts and tutorials for implementing semantic image search on the Cloudera Machine Learning platform.

---

## Recommended reading

Our research engineers share their favorite reads of the month.

- [What makes fake images detectable?](https://arxiv.org/abs/2008.10588)
Detecting fake images is a constantly evolving problem with a number of ethical considerations. The approach discussed in the paper helps determine whether an image was produced from a camera or doctored from a deep neural network or was partially manipulated. It does so by focussing on textures in the local area in hair, backgrounds, mouths, and eyes, rather than the global semantics of the image. The approach uses a fully-convolutional patch-based classifier to focus on these local patches, and tests on different model resolutions, initialization seeds, network architectures, and image datasets. The idea being that CNNs are sensitive to these textures which makes them well suited for this purpose. While detecting fake images is a cat-and-mouse problem, approaches like these can help practitioners understand where manipulations can occur and better anticipate them. — [*Nisha*](https://twitter.com/NishaMuktewar)
- [Monitoring and Explainability of Models in Production](https://arxiv.org/abs/2007.06299)
Monitoring of deployed machine learning models is a critical (and often overlooked) step in the machine learning lifecycle. The authors of this paper outline four important components following model deployment that help ensure the continued success of machine learning services in production: monitoring model performance, monitoring metrics related to incoming data, detecting outliers and data drift, and explaining model predictions. — [*Andrew*](https://twitter.com/andrew_reed_r)
- [The Visual Display of Quantitative Information](https://www.edwardtufte.com/tufte/books_vdqi)
I recently re-read Edward Tufte's classic treatise on data viz. I do not agree with everything in the book. Over-optimizing on maximizing the data-ink ratio (which the book encourages) sacrifices understanding, and Tufte presents a "preferred form" of a quartile chart I find absurd. Nonetheless, there is much wisdom within, and while the book may no longer represent the state of the art, very many data graphics would be substantially improved by adopting its edicts. - [*Chris*](https://twitter.com/_cjwallace)
- [How to Use t-SNE Effectively](https://distill.pub/2016/misread-tsne/)
While the UMAP algorithm is quickly becoming ubiquitous for dimensionality reduction and visualization, many applications still use the traditional t-SNE approach.  The flexibility of the t-SNE algorithm provides usability, but also complexity. t-SNE figures can be difficult to interpret, sometimes displaying structure that doesn't exist, or masking true relationships as noise. This classic [Distill](https://distill.pub/about/) article details best practices for using t-SNE through a series of gorgeous illustrations on simple datasets, and provides tips on how to develop your t-SNE intuition. — [*Melanie*](www.linkedin.com/in/melanierbeck)
- [Making Sandspiel](https://maxbittker.com/making-sandspiel)
I've been working on learning WebGL and I've myself returning often to Max Bittker's write-up on how he made the web-based falling sand game Sandspiel. [Sandspiel](https://sandspiel.club/) is wonderful in its own right, but what I love about the write-up is that it's a pragmatic and human account of building an application that incorporates some pretty cutting-edge web technologies (Rust compiled to WebAssembly for particle simulation and WebGL shaders for fluid simulation). I think it captures the process of prototyping something, where you're perpetually experimenting and adjusting, really well. — [*Grant*](https://twitter.com/grantcuster)
- [Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey](https://arxiv.org/abs/1902.06162)
****Self-supervised and unsupervised learning methods allow us train models without the costs associated with data annotation. This extensive review covers the motivations, general pipelines and methods that have been used for self-supervised visual feature learning. I find the treatment on strategies used to construct **pretext tasks particularly useful. These strategies can be adapted in deriving semantic representations for multiple data domains beyond images e.g. system logs, network traffic sequences, multimodal text and image data, tabular data.  **Pretext tasks are (mostly artificial) tasks designed for a neural network to solve that results in learning useful representations. — [*Victor*](https://twitter.com/vykthur)

---

## Events

Our first ever [Research Roundup](https://www.cloudera.com/about/events/webinars/fast-forward-research-roundup.html) webinar is tomorrow! Join us to hear about our two recent releases: meta-learning, and structural time series. If you can't make it, catch up on-demand later!