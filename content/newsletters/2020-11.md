---
date: 2020-11-20T16:22:30Z
---

Welcome to the November edition of Cloudera Fast Forward's monthly newsletter. This month we have some refreshed research in the computer vision space, a preview of our final report of the year on zero-shot text classification, and our curated best reads and watches of the month from the research team.

---

## New research release!

### Semantic Image Search

![A screenshot of Convnet Playground, a semantic image search app](/images/hugo/semantic-image-search-1605891132.png)

Within this research cycle, we revisited the topic of semantic search on data. We explore two critical requirements for semantic search at scale - a [review of strategies](https://blog.fastforwardlabs.com/2020/11/15/representation-learning-101-for-software-engineers.html) for creating semantic representations of images (supervised, self supervised, unsupervised methods) and provide an implementation of semantic search using fast approximate nearest neighbor search (FAISS).

We have released an updated version of [ConvNet Playground App](https://github.com/cloudera/CML_AMP_Image_Analysis), and a set of scripts and tutorials for implementing semantic image search on the Cloudera Machine Learning platform.

### Research preview: Zero-shot text classification

Text classification is a ubiquitous capability with a wealth of use cases including sentiment analysis, topic assignment, document identification, article recommendation, and more. But collecting enough annotated examples to train traditional classifiers can be quite costly. Instead, we take a look at a classic technique that can be used to perform text classification with few or even zero training examples! We're talking about text embeddings, of course. New advances have significantly increased the quality of document embeddings and in this cycle we'll cover

- how to use them for topic classification,
- best practices for using them,
- and potential limitations.

We'll share code snippets so you can try it for yourself, and we'll release a demo so you can see the method in action! Stay tuned.

---

## Recommended reading

Our research engineers share their favorite reads of the month.

- [Deep Learning Based Text Classification: A Comprehensive Review](https://arxiv.org/abs/2004.03705)
Comprehensive indeed! This group of industry-centric and academic authors surveys over 150 deep learning models developed in the past 6-7 years and provides an overview of more than 40 popular training datasets. They also cover standard text classification performance metrics and compare the performance of dozens of models on a slew of tasks including sentiment analysis, news categorization, question answering, and natural language inference in an apples-to-apples way. While lengthy, this article is clear and concise and is sure to become a useful NLP reference for all things text classification.  — [Melanie](http://www.linkedin.com/in/melanierbeck)
- [What is a Feature Store?](https://www.tecton.ai/blog/what-is-a-feature-store/)
Feature stores have emerged as a necessary component in the operational machine learning tool stack. This article describes the key components of a modern feature store and how the sum of these parts act as a force multiplier on organizations - by reducing duplication of data engineering efforts, speeding up the machine learning lifecycle, and unlocking a new kind of collaboration across data science teams. — [Andrew](https://www.linkedin.com/in/andrew-r-reed/)
- [Principles of good machine learning system design](https://www.youtube.com/watch?v=c_AUuTuPA5k&t=961s)
Until a few years back the tools and frameworks in the ML space were focussed on data cleaning, feature engineering, exploratory analysis and modeling - especially in terms of algorithms or techniques that emphasized model performance. While some of it still prevails, recent years have increasingly witnessed a shift in focus to the operations side of it - how to build and deploy end-to-end ML systems along with the challenges that the industry faces while doing so. A recent [talk](https://www.youtube.com/watch?v=c_AUuTuPA5k&t=962s) from Stanford MLSys Seminar provides a great overview of what it means to operationalize ML models.

  It starts with the differences between working with ML in research versus production in terms of the goal(s), computation, data, fairness and interpretability aspects alongside debunking some myths in this space. A couple of them particularly resonated with us:

  - "You don't need to update your models as much"
  - "Most ML engineers don't need to worry about scale "

  The speaker also shared an iterative framework for ML systems design, covering various aspects from product management, data management, model development, deployment, monitoring and maintenance, integrating it with the business, including differences in data scientists and ML engineers and where they sit within the organization. And finally concluding with four phases of ML adoption. Overall, unlike the DevOps world, there are no universal solutions yet, a lot of interesting new frameworks and real-world lessons to be learnt! — [*Nisha*](https://twitter.com/NishaMuktewar)

- [Underspecification Presents Challenges for Credibility in Modern Machine Learning](https://arxiv.org/abs/2011.03395)
This excellent paper is the latest manifestation of an emerging trend. Machine learning, when deployed for real, must confront the fact that the i.i.d. assumption simply does not hold in practice. In this case, the authors perform a series of compelling experiments in diverse domains of ML application, showing that machine learning pipelines are commonly underspecified - meaning many equivalent models (initializing weights with different random seeds, for instance) would give the same results on the test case. However, each of these models has vastly different performance characteristics if the data at deploy time is not i.i.d. with the training data. The paper is both a call for significantly better stress testing of ML models, and resounding evidence of the opportunity for causal inference (where non-i.i.d. data is addressed explicitly) to improve machine learning practice. — [Chris](https://twitter.com/_cjwallace)
- [NeuralQA: A Usable Library for Question Answering (Contextual Query Expansion + BERT) on Large Datasets](https://www.aclweb.org/anthology/2020.emnlp-demos.3.pdf)
It can be challenging to design  QA systems that integrate with existing search infrastructure (e.g ElasticSearch, Solr) while offering helpful defaults for a set of related QA tasks. In this work we introduce NeuralQA - an easy to use GUI based library for open domain QA. NeuralQA implements helpful defaults (e.g. using masked language models to suggest useful query expansion terms, compression of lengthy documents, model explanations). It is configurable via a yaml config file, can be imported as a python class or called via REST APIs and provides a GUI. It plays well with existing industry standard tools (HuggingFace NLP QA models, ElasticSearch/Solr for retrieval) and is open source! — [Victor](https://twitter.com/vykthur)