---
date: 2021-01-20T16:14:00Z
---

Welcome to the January edition of Cloudera Fast Forward's monthly newsletter.

---

## Top ML research developments of 2020

In lieu of new research of our own this month, we asked each of our research engineers to take a look back at their top research developments of 2020.

### The prevalence of MLOps

[_Andrew_](https://www.linkedin.com/in/andrew-r-reed/)

It has long been known that world of machine learning lacks the rigor and discipline of traditional software engineering practices. In 2020, as more organizations matured in ML capability from experimentation towards integrated production systems, the need for such discipline has become increasingly apparent. Scaling ML effectively is difficult and requires deliberate capacity for model and data lifecycle management, versioning and iteration, governance, release integration, monitoring, and testing.

Consequently, [MLOps](https://cloud.google.com/solutions/machine-learning/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning) - an engineering culture and practice that aims to unify ML system development and ML system operations - has taken [a strong hold within the ML community](https://mlops.community/). MLOps advocates for automation and monitoring at all steps of system construction, and while a [slew of tools and technologies](https://github.com/kelvins/awesome-mlops) have emerged to help satiate the needs of production ML, attention must also be paid to the formation of ML roles, teams, and processes.

In the year ahead, I hope to see the practice of MLOps shift from a nice-to-have afterthought towards an upfront requirement for ML projects, as well as the standardization and alignment of tools and tasks involved.

### Language models are bigger than ever — but are they _better_?

[_Melanie_](http://www.linkedin.com/in/melanierbeck)

NLP models are just getting ridiculous.

The past twelve months have seen records repeatedly shattered: the size of language models continues to grow exponentially and seemingly without bound. Last February, Microsoft announced what was, at the time, the largest language model ever trained. Named [Turing-NLG](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/), it consists of 17 billion parameters. But fanfare was overshadowed by the unveiling in May of [GPT-3,](https://arxiv.org/abs/2005.14165) OpenAI's eye-popping 175 BILLION parameter behemoth. This model could [seemingly](https://venturebeat.com/2020/06/01/ai-machine-learning-openai-gpt-3-size-isnt-everything/) do it all, writing poems, op-eds, and even working code! But just last week, [Google raised the stakes](https://thenextweb.com/neural/2021/01/13/googles-new-trillion-parameter-ai-language-model-is-almost-6-times-bigger-than-gpt-3/) again with the [Switch Transformer](https://arxiv.org/pdf/2101.03961.pdf), describing a language model trained with ONE TRILLION parameters.

While these models exhibit jaw-droppingly impressive capabilities, this arms race of size harbors a disquieting number of growing ethical concerns:

- training these models costs MILLIONS of dollars, ensuring that only the most wealthy tech companies can create them, calling into question the "democratization" of NLP
- training requires massive amounts of electricity from typically non-renewable sources, leading some to question their worth from a standpoint of climate change
- and perhaps most concerning of all, language models are [rife](https://venturebeat.com/2020/04/22/stereoset-measures-racism-sexism-and-other-forms-of-bias-in-ai-language-models/) with [biases](https://thenextweb.com/neural/2021/01/19/gpt-3-has-consistent-and-creative-anti-muslim-bias-study-finds/) against marginalized groups in often subtle and pernicious ways

At Cloudera Fast Forward, we've always prioritized the ethics of machine learning, and while this year has seen the rise of the Mega-Models, my hope is that in 2021 the NLP community begins to more rigorously address the ethical concerns around training and using large language models. While such efforts have begun, one key to solidifying change would be an agreement within the community to prioritize new metrics and standards for the quality of language models, rather than merely competing for leaderboard accuracy.

### Bridging the gap between research and production

[_Nisha_](https://twitter.com/NishaMuktewar)

While machine learning continues to make strides on what can be accomplished with state-of-the-art research approaches, it has been increasingly evident that the industry faces tremendous challenges when it comes to deploying these applications. These challenges include - managing data pipelines, building end-to-end platforms for developing and deploying ML applications, model monitoring, building chips optimized for ML algorithms and inference and even more fundamental issues surrounding ethics and fairness.

In 2021 and coming years, I hope academia and the ML community as a whole can rise and drive focus to these interesting questions, especially when it comes to standardizing tools and frameworks and thus democratizing access to ML in a true sense. That said, we have already started to see multiple venues a practitioner could learn from including [Stanford’s MLSys Seminar Series](https://mlsys.stanford.edu/) and ICML’s workshop - [Challenges in Deploying and Monitoring ML Systems](https://icml.cc/Conferences/2020/ScheduleMultitrack?event=5738).

### Advances in self supervised learning

[_Victor_](https://twitter.com/vykthur)

Many real world problems are characterized by the availability of large datasets but few labels. Self supervised learning approaches such as contrastive learning, self training and generative modeling, provide practical pathways for learning from these data sources while avoiding costs associated with labelling. Two of the more interesting applications of self supervised learning I saw last year include [CLIP](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language.pdf) - a model that learns visual concepts from natural language supervision and [SWAV](https://arxiv.org/abs/2006.09882) a model that learns visual features by cluster assignments. Both methods introduce new formulations of contrastive learning (applicable to multiple problem domains) and achieve performance at par with fully supervised learning models! For more information, see the the self supervised learning section of our recent blog post - [Representation Learning 101 for Software Engineers](https://blog.fastforwardlabs.com/2020/11/15/representation-learning-101-for-software-engineers.html).

### The causal revolution continues

[_Chris_](https://twitter.com/_cjwallace)

Causal inference has been on the periphery of the collective machine learning research agenda for some time, with some high profile debates in the respective causal and ML research communities (see, for instance, [Towards Clarifying the Theory of the Deconfounder](https://arxiv.org/abs/2003.04948)). While no _single_ event last year marked a breakout success of causal reasoning in machine learning, it continues to gather attention, with Causal Learning featuring as the Breiman Lecture at NeurIPS, alongside dedicated workshops at several major conferences.

My favourite related work of the year was [Underspecification Presents Challenges for Credibility in Modern Machine Learning](https://arxiv.org/abs/2011.03395). It points out a very practical problem: that highly parametrized models are very vulnerable to the kind of distributional changes that occur when moving between training and production deployment. The authors identify model _underspecification_ as the source, and, while only barely talking about causality, offer a gateway into causal reasoning as a route to robustness (to my mind, the paper clearly illustrates the need for both).

At some point in the past, I viewed causal reasoning as academic and impractical. In the year 2020 it became abundantly clear to me that causal reasoning carries enormous implications for real world, deployed machine learning systems. As businesses continue to adopt ML-enabled systems, the need for understanding _beyond correlation_ will only grow.

Much of the introductory causal learning literature is written for people with a background in statistical inference, rather than predictive machine learning systems. If you're intrigued by causality, but don't know where to start, I humbly suggest our report [Causality for Machine Learning](https://ff13.fastforwardlabs.com/), which provides an on-boarding suitable for data scientists, machine learning engineers and technology leaders.

![A causal question: which came first, the chicken or the egg?](/images/hugo/chicken-or-egg-1611159168.png)

That's all from us this month. Thanks for reading!
