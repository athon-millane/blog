---
author: Gene Kogan
author_link: http://www.genekogan.com/
date: "2016-02-16T16:35:11Z"
post_type: Guest Post
preview_image: http://68.media.tumblr.com/2cf1b0924a1bfc66737d96f4797ef6f7/tumblr_inline_o2neravFOu1ta78fg_540.png
redirect_from:
- /post/139428219748/machines-and-metaphors
tags:
- whitepaper
- machine learning
- art
- deep learning
title: Machines and Metaphors
---

<figure data-orig-width="703" data-orig-height="376" class="tmblr-full"><img src="http://68.media.tumblr.com/2cf1b0924a1bfc66737d96f4797ef6f7/tumblr_inline_o2neravFOu1ta78fg_540.png" alt="image" data-orig-width="703" data-orig-height="376"/></figure>

##### This is a guest post by <a href="http://www.genekogan.com/">Gene Kogan</a>, an artist and programmer who applies emerging technology into artistic and expressive contexts, and teaches courses and workshops on topics related to code and art.

<p>Recent advances in deep learning research have renewed popular interest in machine intelligence. With new benchmarks set in tough problems (e.g., image classification and speech recognition), researchers are exploring unexpected and exciting applications, and eliciting public engagement and private investment. These recent breakthroughs have captured the attention of many for whom AI was previously obscure, as new capabilities spur applications of interest to wider public audiences.<br/></p><p>But these advances have captured more than just our attention; they&rsquo;ve captured our  imagination. Artists have been quick to apply these new techniques for novel creations, exploring the uncharted territories of machine creativity, slyly provoking questions of greater importance. What is creativity anyway? How do machines perceive, learn, and imitate?</p><!--more--><p>When an algorithm is taught to paint the Mona Lisa in the style of van Gogh’s Starry Night, it doesn&rsquo;t just demonstrate an ability to paint like van Gogh; it demonstrates a much more general ability to emulate human behavior. Taken further, it shows the capacity of an algorithm to take bits of seemingly disorganized and meaningless morsels of data&ndash;pixels, characters&ndash;and abstract knowledge from them which is quite meaningful to humans. Even the world&rsquo;s foremost researchers are admittedly awestruck by these results. We may understand the math, but the intuition for <a href="http://blog.fastforwardlabs.com/2015/09/02/dalemberts-deep-dream-bees-and-nonlinear.html">why it works escapes us</a>.</p><figure data-orig-width="558" data-orig-height="276" class="tmblr-full"><img src="http://68.media.tumblr.com/3cbe548003e0178e5a8bc834ca134f6b/tumblr_inline_o2nesdb9uq1ta78fg_540.png" alt="image" data-orig-width="558" data-orig-height="276"/></figure>

##### <a href="https://github.com/jcjohnson/neural-style">neural-style</a> of Mona Lisa in cubist, expressionist, and impressionist form

<p>Artistic applications can help mitigate this uncertainty, and naturally, many are initiated within the research community itself, sometimes with intentionally creative undertones. <a href="http://googleresearch.blogspot.in/2015/06/inceptionism-going-deeper-into-neural.html">Deepdream</a> and <a href="http://arxiv.org/abs/1508.06576">style transfer</a> (or &ldquo;neural style&rdquo;, &ldquo;#<a href="https://twitter.com/search?f=images&amp;vertical=default&amp;q=%23stylenet">stylenet</a>&rdquo;) were publicly released by researchers, piquing the curiosity of many practitioners online (including <a href="http://genekogan.com/works/style-transfer.html">myself</a>) who riffed on the software to create countless artworks. Both received a great deal of mainstream press coverage [<a href="http://www.wired.co.uk/news/archive/2015-07/03/google-deep-dream">1</a>][<a href="http://www.telegraph.co.uk/technology/google/11730050/deep-dream-best-images.html">2</a>][<a href="http://www.popsci.com/turn-your-life-computers-dream-world">3</a>][<a href="http://www.theguardian.com/technology/2015/sep/02/computer-algorithm-recreates-van-gogh-painting-picasso">4</a>][<a href="http://qz.com/495614/computers-can-now-paint-like-van-gogh-and-picasso/">5</a>][<a href="http://www.dailymail.co.uk/sciencetech/article-3214634/The-algorithm-learn-copy-artist-Neural-network-recreate-snaps-style-Van-Gogh-Picasso.html">6</a>], becoming among the first examples of machine-learned generative art being shown to wider audiences. Using <a href="http://cs.stanford.edu/people/jcjohns/">Justin Johnson</a>&rsquo;s popular <a href="https://github.com/jcjohnson/neural-style">neural-style</a> implementation, I produced a <a href="http://vimeo.com/139123754">restyled version</a> of a scene from Alice in Wonderland. At the time, the results seemed too good to be real, but I had a hunch that more clarity would arrive downstream.<br/></p><p>The <a href="https://github.com/Newmu/dcgan_code">implementation</a> of a <a href="http://arxiv.org/abs/1511.06434">deep convolutional generative adversarial network</a> <a href="http://arxiv.org/abs/1511.06434">(</a>D<a href="http://arxiv.org/abs/1511.06434">C</a>G<a href="http://arxiv.org/abs/1511.06434">A</a>N<a href="http://arxiv.org/abs/1511.06434">)</a> published on Github by <a href="https://twitter.com/alecrad">Alec Radford</a>, <a href="http://soumith.ch">Soumith Chintala</a>, and <a href="http://lukemetz.github.io/">Luke Metz</a> is a good example of research partially motivated by creative goals. Describing their algorithm as &ldquo;tripping&rdquo; and packing the <a href="https://github.com/Newmu/dcgan_code/blob/master/README.md">README</a> with troves of machine-hallucinated psychedelia, they seemed to be deliberately inviting artists to repurpose their code. So I took the bait and trained a DCGAN to generate <a href="http://www.genekogan.com/works/a-book-from-the-sky.html">animated interpolations of handwritten Chinese characters</a>. Others applied the technique to produce <a href="https://twitter.com/vintermann/status/675599478494208000">fake flowers</a> and <a href="https://github.com/mattya/chainer-DCGAN/blob/master/README.md">manga characters</a>, eliciting <a href="https://www.facebook.com/yann.lecun/posts/10153269667222143">praise from Yann LeCun</a> and others.</p><figure data-orig-width="512" data-orig-height="314" class="tmblr-full"><img src="http://68.media.tumblr.com/450487b441fdae6b17b5fe7a75e562e9/tumblr_inline_o2nf2jx1V41ta78fg_540.gif" alt="image" data-orig-width="512" data-orig-height="314"/></figure>

##### excerpt from <a href="http://genekogan.com/works/a-book-from-the-sky.html">A Book from the Sky</a>

<p>Spurred by these and others, more creative machine learning hacks have appeared in recent months, including another project involving Chinese characters by <a href="https://twitter.com/hardmaru">@hardmaru</a>. Here, the artist <a href="http://blog.otoro.net/2015/12/28/recurrent-net-dreams-up-fake-chinese-characters-in-vector-format-with-tensorflow/">invented wholly new characters</a> from a trained <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">recurrent neural network</a>. Continuing with the theme of written language aesthetics, <a href="http://erikbern.com">Erik Bernhardsson</a> explored the <a href="http://erikbern.com/2016/01/21/analyzing-50k-fonts-using-deep-neural-networks/">latent space of typography</a>, training a network on 50,000 fonts. Another strain of research, combining convolutional and recurrent neural networks, has produced software capable of annotating or describing images with natural language [<a href="https://github.com/ryankiros/neural-storyteller">1</a>][<a href="https://github.com/karpathy/neuraltalk2">2</a>], and even more experimentally, the <a href="http://arxiv.org/abs/1511.02793">reverse</a>. These hint toward a future in which machines can autonomously process information multi-modally, exchanging text for images or sounds, and vice-versa. These implementations reflect a growing interest of those with more technical backgrounds to apply their research in artistic and design-focused ways.</p><figure data-orig-width="512" data-orig-height="512" class="tmblr-full"><img src="http://68.media.tumblr.com/569a9c7fdf3a1db183d42b7b91389759/tumblr_inline_o2nlv9YJ041ta78fg_540.gif" alt="image" data-orig-width="512" data-orig-height="512"/></figure>

##### excerpt from Erik Bernhardsson’s <a href="http://erikbern.com/2016/01/21/analyzing-50k-fonts-using-deep-neural-networks/">font-generating neural net</a>

<p>Conversely, those coming from artistic backgrounds have been delving deeper into scientific literature, reading public papers on <a href="http://arxiv.org/">arxiv.org</a>, and designing their own deep neural networks using open-source frameworks like <a href="http://caffe.berkeleyvision.org/">Caffe</a>, <a href="http://deeplearning.net/software/theano/">Theano</a>, <a href="http://torch.ch">Torch</a>, and <a href="https://www.tensorflow.org/">TensorFlow</a>, which have dramatically eased the process of getting started. Even higher-level libraries on top of these frameworks have appeared, including <a href="http://keras.io/">Keras</a>, <a href="http://lasagne.readthedocs.org/en/latest/">Lasagne</a>, <a href="https://blocks.readthedocs.org/en/latest/">Blocks</a>, and <a href="http://venturebeat.com/2015/11/14/deep-learning-frameworks/">others</a>. The tools of the trade for artists and scientists alike have been converging, blurring the distinctions between them and facilitating new lines of inquiry and cross-disciplinary dialogue.<b><br/></b></p><p>The rise of these powerful tools has precipitated an explosion in public discourse about machine learning. The <a href="https://www.reddit.com/r/MachineLearning/">ML subreddit</a>, once a treasure chest of wild speculation, is now rife with students and amateurs asking technical questions and sharing project ideas. Many of the authors of the libraries mentioned above actively maintain blogs where they discuss the latest discoveries, often sharing code for how to reproduce them.</p><p>One researcher, <a href="http://karpathy.github.io/">Andrej Karpathy</a> of <a href="http://vision.stanford.edu/">Stanford</a> and <a href="https://openai.com">OpenAI</a>, even released a Javascript implementation of a convolutional neural network, <a href="http://cs.stanford.edu/people/karpathy/convnetjs/">convnet.js</a>. One may wonder why, since Javascript&rsquo;s limitations give this library little applicability to scientific research. But that isn&rsquo;t the point. Embedded in a web page, the neural network is instantly accessible: anyone with an internet connection can now interact with an algorithm that has revolutionized computer vision and speech processing.</p><p>As these new methods improve and mature, they will rapidly be applied in contexts that could have significant consequences for society, raising the stakes for their use. This creates more urgency for outreach and education, <a href="https://medium.com/@genekogan/from-pixels-to-paragraphs-eb2763da0e9b">to inform the general public</a> about these multifaceted and counterintuitive technologies. As exciting as they are, they also risk misuse. The example usually cited is <a href="https://www.technologyreview.com/s/542626/why-self-driving-cars-must-be-programmed-to-kill/">how a self-driving car would choose which of two pedestrians to kill</a> if it could only avoid one. Although this scenario is a bit exaggerated, it’s an example of a decision a machine would inevitably have to make. As implementation decisions along the way introduce biases and affect the outcome, the public should have a say in influencing their development.<br/></p><p>Fortunately, the deep learning research community largely conducts research openly, embracing open-review platforms like arxiv and publishing open source software. Additionally, libraries wrapping machine learning functionality into creative coding toolkits like <a href="http://openframeworks.cc">openFrameworks</a> provide an avenue for artists to probe deeper.</p><figure data-orig-width="551" data-orig-height="305" class="tmblr-full"><img src="http://68.media.tumblr.com/0e261a22a3eae842e7561d81578d9312/tumblr_inline_o2nfbdG2w11ta78fg_540.png" alt="image" data-orig-width="551" data-orig-height="305"/></figure><p><b></b></p>

##### images of <a href="http://www.vision.caltech.edu/Image_Datasets/Caltech256/images/">animals</a> clustered in 2D using <a href="http://openframeworks.cc">openframeworks</a> libraries <a href="https://github.com/kylemcdonald/ofxCcv">ofxCcv</a> and <a href="https://github.com/genekogan/ofxTSNE">ofxTSNE</a>

<p>But the fact that tools are open source does not mean the public understands what those tools entail. Along the journey from initial scientific research and to mass deployment of machine intelligence, artists can help illuminate the gap, providing accessible and engaging cultural metaphors which are more readily understandable than the layers of abstraction in pure research. The machine metaphor has been successful in the past, <a href="https://medium.com/@genekogan/machine-learning-for-artists-e93d20fdb097">helping to popularize computer vision</a> in the context of interactive installations, celebrating its playful side while simultaneously raising caution about its lesser known properties. If present trends continue into the near future, machine intelligence could follow a similar path.<br/></p><p>- Gene Kogan</p>
