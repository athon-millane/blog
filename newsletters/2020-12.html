<!DOCTYPE html>
  <html lang="en">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style type="text/css">
    body{font-family:sans-serif;font-size:16px;line-height:1.5}
    img{max-width:100%; display:block}
    h5{font-style:italic}
    blockquote{border-left:.25em solid #dfe2e5;padding:0 1em;color:#666;margin:1em 0}
    #logo{display: block; height: 1.75rem; margin-top: 20px; margin-bottom: 24px;}
  </style>
  <body>
  <div style="max-width: 660px; margin: 0 auto; padding: 0 12px 24px;">

    <div style="overflow: hidden; font-size: 14px; margin-top: 14px;">
      <div style="float: left; width: 46%;">Updates from Cloudera Fast Forward on new research, prototypes, and exciting developments</div>
        <div style="float: right; width: 46%; text-align: right;"><a href="https://blog.fastforwardlabs.com/newsletters/2020-12.html">View this email in browser</a></div>
      </div>
      <div>
      <img id="logo" src="https://blog.fastforwardlabs.com/images/cloudera-fast-forward-logo.png" />
      </div>
    <p>Welcome to the December edition of Cloudera Fast Forward&rsquo;s monthly newsletter. We have a bumper pack of releases for the holiday season: a new research release, the open sourcing of <em>three</em> previous reports, and, as usual, our team&rsquo;s recommended reading for the month.</p>
<hr>
<h2 id="new-research-release">New research release!</h2>
<h3 id="few-shot-text-classification">Few-Shot Text Classification</h3>
<p><img src="/images/hugo/bert_and_w2v-1607641509.png" alt="BERT and Word2Vec discuss text classification"></p>
<p>Text classification is a ubiquitous capability with a wealth of use cases including sentiment analysis, topic assignment, document identification, article recommendation, and more. But collecting enough annotated examples to train traditional classifiers can be quite costly. Instead, we take a look at a classic technique that can be used to perform text classification with few or even zero training examples! We&rsquo;re talking about text embeddings, of course. New advances have significantly increased the quality of document embeddings and in our newest writing on <a href="https://few-shot-text-classification.fastforwardlabs.com">Few Shot Text Classification</a> this cycle we cover</p>
<ul>
<li>how to use them for topic classification,</li>
<li>best practices for using them,</li>
<li>and potential limitations.</li>
</ul>
<p>Follow the links in the report to find code snippets so you can try it for yourself, and build your own demo so you can see the method in action!</p>
<h2 id="federated-learning-open-source">Federated Learning open source</h2>
<p><img src="/images/hugo/federated_cover-1607641562.png" alt="The Federated Learning report cover"></p>
<p>Two years ago we wrote a research report about Federated Learning. We’re pleased to make the report available to everyone, for free. You can read it online here: <a href="https://federated.fastforwardlabs.com/">Federated Learning</a>.</p>
<p>In the time since, it has only grown in relevance. Numerous startups have cropped up (and some disappeared by acquisition) with Federated Learning as their core technology. Google continues to promote the technology, including for non-machine learning use cases, as in <a href="https://ai.googleblog.com/2020/05/federated-analytics-collaborative-data.html">Federated Analytics: Collaborative Data Science without Data Collection</a>. This year saw (what we believe to be) the first conferences with a heavy focus on federated learning, <a href="https://federatedlearningconference.com/">The Federated Learning Conference</a> and the <a href="https://blog.openmined.org/openmined-privacy-conference-2020/">Open Mined Privacy Conference</a>, as well as dedicated workshops at high profile machine learning conferences like <a href="http://federated-learning.org/fl-icml-2020/">ICML</a> and <a href="https://nips.cc/Conferences/2020/ScheduleMultitrack?event=16123">NeurIPS</a>.</p>
<p><a href="https://www.openmined.org/">OpenMined</a> continues to build a strong community around private machine learning, creating <a href="https://www.udacity.com/course/secure-and-private-ai--ud185">courses</a> and <a href="https://github.com/OpenMined">open source tools</a> to lower the barrier-to-entry to federated learning and related privacy enhancing techniques. Alongside those, <a href="https://www.tensorflow.org/federated">TensorFlow Federated</a>, <a href="https://github.com/IBM/federated-learning-lib">IBM’s federated learning library</a> and <a href="https://flower.dev/">flower.dev</a> are extending the tooling ecosystem.</p>
<p>Federated Learning is no panacea. In a privacy setting, decentralized data simply presents a different attack surface to centralized data. Not all applications require or benefit from federation. However, it <em>is</em> an important tool in the private machine learning toolkit.</p>
<h2 id="deep-learning-for-image-analysis">Deep Learning for Image Analysis</h2>
<p>To accompany last month&rsquo;s research on Semantic Image Search (checkout the associated blog post Representation Learning 101 for Software Engineers), we&rsquo;re opening up some more previous reports:</p>
<ul>
<li><a href="https://deep-learning-image-classic.fastforwardlabs.com/">Deep Learning for Image Analysis</a> is an oldie, having been released back in 2015, but still provides an introduction for the uninitiated.</li>
<li>Our more recent release on the same, <a href="https://deep-learning-image-analysis.fastforwardlabs.com/">Deep Learning for Image Analysis: 2019 edition</a>, substantially expands the first report and covers some practical considerations, like trading off accuracy and latency, and interpreting model predictions. July 2019 is a long time ago in computer vision research, and while the benchmarks may have improved, the underlying concepts discussed are still relevant.</li>
</ul>
<hr>
<h2 id="recommended-reading">Recommended reading</h2>
<p>Our research engineers share their favorite reads of the month.</p>
<ul>
<li><a href="https://arxiv.org/abs/2005.04118">Beyond Accuracy: Behavioral Testing of NLP Models with CheckList</a>
As a ML practitioner, one of the primary ways of evaluating a model’s performance is through measuring it’s accuracy on a held-out test dataset. While this is a useful indicator, time and again it has been recognized that such an approach does not necessarily mean that the model will generalize well in a production environment. Further, such a performance indicator provides little insight into where the model fails, and how one could fix it. So the question is how could one check whether a model really works? This is where one could borrow from software engineering practices. The authors (also the authors of the popular model explainability tool - LIME) propose CheckList, a task and model-agnostic methodology for testing NLP models inspired by principles of behavioral testing in software engineering. CheckList guides users in what to test by suggesting a list of linguistic capabilities. For example, testing vocabulary of the model or how it deals with named entities or negation. Further,  it tests for potential capability failures by introducing different test types, such as prediction invariance in presence of certain perturbations. In addition, it provides users with tooling to generate hundreds of test cases easily using templates, lexicons,  general purpose perturbations, visualizations and others. The authors also illustrate it’s utility by highlighting significant problems for some SOTA models and reveal bugs in commercial systems developed by large software companies. More importantly, it can help come up with a standardized way of evaluating NLP models moving beyond just accuracy on held-out datasets!  — <a href="https://twitter.com/NishaMuktewar"><em>Nisha</em></a></li>
<li><a href="https://nips.cc/virtual/2020/public/invited_16166.html">You Can&rsquo;t Escape Hyperparameters and Latent Variables: Machine Learning as a Software Engineering Enterprise</a>
In his keynote presentation at NeurIPS 2020, Charles Isbell delivers a video montage conveying a stark reality of applied machine learning today - the field of ML is too narrowly focused on solutions without a broader consideration for their impact on the real human world. By sharing perspectives from multiple researchers, Charles Isbell makes the argument that the community needs to adopt systematic approaches rooted in software engineering, programming languages, ethics, and diversity to design and build more robust, holistic machine learning systems. — <a href="https://www.linkedin.com/in/andrew-r-reed/"><em>Andrew</em></a></li>
<li><a href="https://arxiv.org/abs/2003.04948">Towards Clarifying the Theory of the Deconfounder</a>
Back when we wrote our report <a href="https://ff13.fastforwardlabs.com/">Causality for Machine Learning</a>, I spent some time with the so-called &ldquo;deconfounder&rdquo; model of Wang and Blei. The original paper, <a href="https://arxiv.org/abs/1805.06826">The Blessings of Multiple Causes</a> landed with a splash, generating much excitement in the machine learning community and skepticism among causal inference researchers. In a fantastic example of science in action, numerous subsequent papers have clarified the theory and use of the proposed technique. The essence of the original paper is that by constructing a latent factor model of multiple causes, under some assumptions, we can identify and estimate causal effects under weaker assumptions than usual. It turns out that there are additional assumptions in the new method too, but it does provide a new tool for causal inference in some scenarios. I remain convinced that causality is a necessary framework for machine learning in industry, and carries many benefits even when we care only about prediction. — <em><a href="https://twitter.com/_cjwallace">Chris</a></em></li>
<li><a href="https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology">AlphaFold: a solution to a 50-year-old grand challenge in biology</a>
This interactive article tells what seems like a fairy-tale story of how AI saves the day in the world of biology. Only it&rsquo;s not a fairy tale! Fifty years ago a Nobel Prize-winning chemist predicted that a protein&rsquo;s amino acid sequence should fully determine the shape of its structure.  Knowing a protein&rsquo;s shape and structure can provide insights into new diseases, allowing for better drug development and treatments. Unfortunately, predicting protein shapes is an astronomically difficult challenge. Known as the &ldquo;protein folding problem,&rdquo; one estimate is that any given protein could have 10^300 different possible configurations.  Searching by brute force for the right structure would take longer than the age of the Universe! Until now. DeepMind&rsquo;s AlphaFold has recently been recognized as a solution for the protein folding problem and demonstrates how AI can have a deeply meaningful impact on the life sciences and scientific discovery. — <a href="www.linkedin.com/in/melanierbeck"><em>Melanie</em></a></li>
<li><a href="https://www.penguinrandomhouse.com/books/538034/exhalation-by-ted-chiang/">Exhalation</a> - Ted Chiang
Exhalation is the second collection of short fiction from the author of Story of Your Life (adapted into the hit sci-fi film Arrival). The stories are the best kind of sci-fi, being both cerebral and grounded in a profound appreciation of the human experience. — <em><a href="https://twitter.com/_cjwallace">Chris</a></em></li>
<li><a href="https://www.penguinrandomhouse.com/books/600671/how-to-do-nothing-by-jenny-odell/">How to Do Nothing</a> - Jenny Odell
This book from Bay Area-based installation and internet artist Odell was recommended to me several times during the year of its publishing (2019). It wasn&rsquo;t until partway through 2020 (a year in which it&rsquo;s felt at times like we are all are being forced to do nothing) that I finally picked it up. I&rsquo;m glad I did. Odell offers an antidote to doomscrolling that is both permission to step back from engaging with contemporary concerns and a call to action to pay attention to ourselves and each other in new ways. I&rsquo;m enjoying this one slowly, with a long walk and a few days between each chapter. — Juno</li>
</ul>

  </div>


